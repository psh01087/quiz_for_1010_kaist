{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To ignore deprecated warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from torch.utils import data # pytorch data class \n",
    "import torch.nn as nn # pytorch neural network 불러오기 \n",
    "import torch.nn.utils.rnn as rnn_utils # rnn utils\n",
    "import torch\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(seqs):\n",
    "    num_skip_sent = 0\n",
    "    word_count = 4\n",
    "    vocab = [\"<pad>\",\"<s>\",\"</s>\",\"<unk>\"]\n",
    "    word2id = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3}\n",
    "    id2word = {0: \"<pad>\", 1: \"<s>\", 2: \"</s>\", 3: \"<unk>\"}\n",
    "    print(\"Building vocab and dict..\")\n",
    "    for line in seqs:\n",
    "        words = line.strip().split(' ') # tokenized by space \n",
    "        for word in words:\n",
    "            if word not in vocab:\n",
    "                word_count += 1 # increment word_count\n",
    "                vocab.append(word) # append new unique word\n",
    "                index = word_count - 1 # word index (consider index 0)\n",
    "                word2id[word] = index # word to index\n",
    "                id2word[index] = word # index to word\n",
    "    \n",
    "    print(\"Number of unique words: %d\" % len(vocab))\n",
    "    print(\"Finised building vocab and dict!\")\n",
    "\n",
    "    return vocab, word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(*args)\n",
    "\n",
    "\n",
    "def pad_tensor(vec, pad, value=0, dim=0):\n",
    "    \"\"\"\n",
    "    pad token으로 채우는 용도 \n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = pad - vec.shape[0]\n",
    "\n",
    "    if len(vec.shape) == 2:\n",
    "        zeros = torch.ones((pad_size, vec.shape[-1])) * value\n",
    "    elif len(vec.shape) == 1:\n",
    "        zeros = torch.ones((pad_size,)) * value\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return torch.cat([torch.Tensor(vec), zeros], dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, values=(0, 0), dim=0):\n",
    "    \"\"\"\n",
    "    데이터 로더에 들어가기전에 batch화 할 때 거치는 함수 \n",
    "    args:\n",
    "        batch - list of (tensor, label)\n",
    "    reutrn:\n",
    "        xs - a tensor of all examples in 'batch' after padding\n",
    "        ys - a LongTensor of all labels in batch\n",
    "        ws - a tensor of sequence lengths\n",
    "    \"\"\"\n",
    "\n",
    "    sequence_lengths = torch.Tensor([int(x[0].shape[dim]) for x in batch]) # 각 batch 마다 길이를 얻어내고 \n",
    "    sequence_lengths, xids = sequence_lengths.sort(descending=True) # 감소하는 순서로 정렬\n",
    "    # find longest sequence (가장 긴 sequence의 길이를 구함 )\n",
    "    max_len = max(map(lambda x: x[0].shape[dim], batch))\n",
    "    # pad according to max_len (max length 만큼 padd를 추가 )\n",
    "    batch = [(pad_tensor(x, pad=max_len, dim=dim), label) for (x, label) in batch]\n",
    "\n",
    "    # stack all\n",
    "    xs = torch.stack([x[0] for x in batch], dim=0)\n",
    "    xs = xs[xids].contiguous() # decreasing order로 다시 나열\n",
    "    \n",
    "    labels = [x[1] for x in batch]\n",
    "    labels = [labels[i] for i in xids] # decreasing order로 다시 나열\n",
    "    \n",
    "    return xs.long(), sequence_lengths.int(), torch.Tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, path , word2id):\n",
    "        self.seqs = open(path).readlines()\n",
    "        self.word2id = word2id\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (source and sentiment).\"\"\"\n",
    "        seqs = self.seqs[index]\n",
    "        seqs, label = self.process(seqs, self.word2id)\n",
    "        return seqs, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "                    \n",
    "    def process(self, seq, word2id):\n",
    "        label = 0 # default label \"pos\"\n",
    "        sequence = []\n",
    "        sequence.append(word2id[\"<s>\"])\n",
    "        words = seq.strip().split(' ')\n",
    "        for i in range(0, len(words)-1):\n",
    "            current_word = words[i]\n",
    "            if \"negative\" == words[len(words)-1]: #if label is \"neg\", then 1\n",
    "                label = 1\n",
    "            if current_word in word2id:\n",
    "                sequence.append(word2id[current_word])\n",
    "            else:\n",
    "                sequence.append(3) # replace by <unk> token\n",
    "        sequence.append(word2id[\"</s>\"])\n",
    "        sequence = torch.Tensor(sequence)\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_trainer(model, optimizer, loaders, epoch,n_epochs):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        model     - sentiment model\n",
    "        optimizer - adam\n",
    "        loaders   - valid, train loaders\n",
    "    return:\n",
    "        model, optimizer\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    total_accuracy = []\n",
    "    val_losses = []\n",
    "    val_total_accuracy = []\n",
    "    n_iter = 0\n",
    "    for split in loaders.keys():\n",
    "        if split == \"train\":\n",
    "            for batch in loaders[split]:\n",
    "                model.train() # train mode\n",
    "                input, input_lengths, target_label = batch\n",
    "                predict_label = model(input.to(device),input_lengths.to(device))\n",
    "                loss_fn = nn.BCELoss().to(device)\n",
    "                loss = loss_fn(predict_label,target_label.to(device))\n",
    "                losses.append(loss.item())\n",
    "                # Calculate accuracy\n",
    "                x_acc = predict_label.round().cpu().detach().numpy()\n",
    "                y_acc = target_label.cpu().detach().numpy()\n",
    "                accuracy = accuracy_score(x_acc,y_acc)\n",
    "                total_accuracy.append(accuracy)\n",
    "                # Reset gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Compute gradients\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                n_iter+=1 # count number of trained sentences\n",
    "                if n_iter % 100 == 0: # print loss only if it's training stage\n",
    "                    print ('\\n [{}] current_iter_loss= {:05.3f} acc= {:05.3f}'.format(n_iter,loss,accuracy))\n",
    "                \n",
    "        elif split == \"valid\":\n",
    "            model.eval() # eval mode\n",
    "            for batch in loaders[split]:\n",
    "                input, input_lengths, target_label = batch\n",
    "                predict_label = model(input.to(device),input_lengths.to(device))\n",
    "                loss_fn = nn.BCELoss().to(device)\n",
    "                loss = loss_fn(predict_label,target_label.to(device))\n",
    "                val_losses.append(loss.item())\n",
    "                # Calculate accuracy\n",
    "                x_acc = predict_label.round().cpu().detach().numpy()\n",
    "                y_acc = target_label.cpu().detach().numpy()\n",
    "                accuracy = accuracy_score(x_acc,y_acc)\n",
    "                val_total_accuracy.append(accuracy)\n",
    "  \n",
    "    print ('\\n Epoch({}/{}) avg train_loss= {:05.3f} train_acc= {:05.3f} val_loss= {:05.3f} val_acc= {:05.3f}'\n",
    "           .format(\n",
    "               epoch+1,n_epochs,np.mean(losses), np.mean(total_accuracy), \n",
    "                   np.mean(val_losses), np.mean(val_total_accuracy)\n",
    "              )\n",
    "          )\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2word(idx, i2w, pad_idx):\n",
    "    \"\"\"\n",
    "    index로 이루어진 문장을 받아,\n",
    "    word 문장으로 전환\n",
    "    \"\"\"\n",
    "\n",
    "    sent_str = [str()]*len(idx)\n",
    "\n",
    "    for i, sent in enumerate(idx):\n",
    "\n",
    "        for word_id in sent:\n",
    "\n",
    "            if word_id == pad_idx:\n",
    "                break\n",
    "            sent_str[i] += i2w[int(word_id)] + \" \"\n",
    "\n",
    "        sent_str[i] = sent_str[i].strip()\n",
    "\n",
    "\n",
    "    return sent_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, epoch,n_epochs, id2word):\n",
    "    \"\"\"\n",
    "    각 epoch 마다 model에 대한 test및, prediction결과 추출\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    total_accuracy = []\n",
    "    model.eval()\n",
    "    for batch in loader:\n",
    "        input, input_lengths, target_label = batch\n",
    "        predict_label = model(input.to(device),input_lengths.to(device))\n",
    "        loss_fn = nn.BCELoss().to(device)\n",
    "        loss = loss_fn(predict_label,target_label.to(device))\n",
    "        losses.append(loss.item())\n",
    "        # Calculate accuracy\n",
    "        x_acc = predict_label.round().cpu().detach().numpy()\n",
    "        y_acc = target_label.cpu().detach().numpy()\n",
    "        accuracy = accuracy_score(x_acc,y_acc)\n",
    "        total_accuracy.append(accuracy)\n",
    "\n",
    "    print ('\\n Epoch({}/{}) avg test_loss= {:05.3f} test_acc= {:05.3f}'\n",
    "        .format(epoch+1,n_epochs,np.mean(losses), np.mean(total_accuracy))\n",
    "    )\n",
    "    \n",
    "    # test inference\n",
    "    print (idx2word(input, id2word, 0))\n",
    "    predict_label = model(input.to(device),input_lengths.to(device))\n",
    "    ans = {0: 'positive', 1:'negative'}\n",
    "    print('\\n Pred: {}, Ans: {}'\n",
    "          .format(ans[predict_label[0].round().item()], ans[target_label[0].item()])\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocab or loading existing vocab\n",
    "path = \"./data/sentiment_id2word.pkl\"\n",
    "if os.path.isfile(path):\n",
    "    with open(\"./data/sentiment_id2word.pkl\", \"rb\") as f:\n",
    "        id2word = pickle.load(f)\n",
    "    with open(\"./data/sentiment_word2id.pkl\", \"rb\") as f:\n",
    "        word2id = pickle.load(f)\n",
    "    with open(\"./data/sentiment_vocab.pkl\", \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "else: # file does not exist\n",
    "    vocab, word2id, id2word = build_dict(seqs)\n",
    "    pickle.dump(vocab, open(\"./data/sentiment_vocab.pkl\", \"wb\" ))\n",
    "    pickle.dump(word2id, open(\"./data/sentiment_word2id.pkl\", \"wb\" ))\n",
    "    pickle.dump(id2word, open(\"./data/sentiment_id2word.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# constructing train dataset\n",
    "train_dataset = Sentiment_Dataset(\"./data/train.tok\", word2id)\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=config[\"batch_size\"],\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "# constructing valid dataset\n",
    "valid_dataset = Sentiment_Dataset(\"./data/valid.tok\", word2id)\n",
    "valid_data_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                          batch_size=config[\"batch_size\"],\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "# constructing test dataset\n",
    "test_dataset = Sentiment_Dataset(\"./data/test.tok\", word2id)\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=config[\"batch_size\"],\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "loaders = {\"train\": train_data_loader, \"valid\": valid_data_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Classification(nn.Module):\n",
    "    \"\"\"\n",
    "    IMDb 영화 리뷰 감성분류 모델\n",
    "    \"\"\"\n",
    "    def __init__(self, config, vocab_size):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            config     - hyperparameters\n",
    "            vocab_size - vocab_size\n",
    "        return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(Sentiment_Classification, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, config[\"embedding_size\"])\n",
    "        self.rnn = nn.GRU(input_size=config[\"embedding_size\"], hidden_size=config[\"hidden_size\"], \n",
    "                          num_layers=config[\"num_layers\"],dropout=config[\"dropout\"],bidirectional = config['bidirectional'] , batch_first=True)\n",
    "        self.bidirectional = config['bidirectional']\n",
    "        self.num_layers = config['num_layers']\n",
    "        if self.bidirectional:\n",
    "            self.hidden_size = 2*config['hidden_size']\n",
    "        else:\n",
    "            self.hidden_size = config['hidden_size']\n",
    "        self.outputs = nn.Sequential(\n",
    "                            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                            nn.LeakyReLU(0.2),\n",
    "                            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                            nn.LeakyReLU(0.2),\n",
    "                            nn.Linear(self.hidden_size, 1),\n",
    "                            nn.Sigmoid()\n",
    "                        )\n",
    "    def forward(self,input, lengths):\n",
    "        #기존 코드를 수정하지 않고 코드를 구현해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"epochs\": ???,\n",
    "  \"hidden_size\": ???,\n",
    "  \"bidirectional\": ???,\n",
    "  \"num_layers\": ???,\n",
    "  \"dropout\": ???,\n",
    "  \"batch_size\": ???,\n",
    "  \"embedding_size\": ???,\n",
    "  \"learning_rate\": ???,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Init sentiment model\n",
    "model = Sentiment_Classification(config, vocab_size=len(vocab))\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model,optimizer  = sentiment_trainer(model,optimizer, loaders, epoch,config[\"epochs\"])\n",
    "    evaluate(model, test_data_loader, epoch,config[\"epochs\"], id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델제출 - 학습된 모델 저장 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 - (./trained_model + ./config.pkl) 제출\n",
    "pickle.dump(config, open(\"./config.pkl\", \"wb\" ))\n",
    "torch.save(model.state_dict(), \"./trained_model\")\n",
    "# 저장된 모델 확인하기\n",
    "if os.path.isfile(\"./config.pkl\"):\n",
    "    with open(\"./config.pkl\",\"rb\") as f:\n",
    "        config = pickle.load(f)\n",
    "model = Sentiment_Classification(config, vocab_size=len(vocab))\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"./trained_model\"))\n",
    "model.eval()\n",
    "evaluate(model, test_data_loader, epoch,config[\"epochs\"], id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
